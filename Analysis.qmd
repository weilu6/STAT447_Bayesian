---
title: "Analysis"
author: "Wei"
format: html
editor: visual
---

## EDA

Read data

```{r}
suppressMessages(require(readxl))
df = read_xlsx('proposal/Data/Pumpkin_Seeds_Dataset.xlsx')
```

Distribution of features

```{r}
suppressMessages(require(ggplot2))
suppressMessages(require(dplyr))
```

```{r}
df$Class <- factor(df$Class)
```

```{r}
features = colnames(df |> select(-"Class"))
```

```{r}
for (feat in features) {
  plot <- df |> ggplot(aes(x=.data[[feat]], fill=Class)) +
    geom_histogram(bins = 30,position = "identity", alpha=0.8) 
  print(plot)
}
```

```{r}
summary(df$Class)
```

### Summary:

-   The observed pumpkin seed classes are nearly balanced
-   All feature values are positive; the distributions are unimodal
-   Except for "Eccentricity", "Solidity", "Extent", the rest all symmetric
-   Generally, the feature values spread wider for "Ürgüp Sivrisi"
-   The disrtibutions of features are shifted between classes.

## preprocessing

The data size is 2500. To evaluate the prediction accuracy, we split the data set into train and test data first, we will do regression on the train data; compare the predictions on the test data
```{r}
# For the implementation of stan, we convert Class into integers;
# 0 for Çerçevelik
# 1 for Ürgüp Sivrisi
df$Class = as.integer(df$Class)-1
```

```{r}
set.seed(123)
trainIndex <- sample(seq_len(nrow(df)), size = 0.8 * nrow(df))
df_train <- df[trainIndex, ]
df_test <- df[-trainIndex, ]

dim(df_train)
dim(df_test)
```

Scaled features data
```{r}
X_train_scale <- scale(select(df_train, -Class))
X_test_scale <- scale(select(df_test, -Class))
```





## Baysian logistic regression with normal priors
Run MCMC
```{r}
suppressMessages(require(rstan))
```
```{r}
options(mc.cores=parallel::detectCores())
```

From EDA, the spread of each features are different, so we choose different priors on standard deviation for features. We use hierarhical model.(Too long to simulate)

#### stan with matrix with prediction
slow mixing, mis-specified
```{stan output.var = "normal_matrix"}
data {
  int N; # number of train data
  int M; # number of test data
  int K; # number of features
  array[N] int y; # label of train
  matrix[N, K] X; # train features
  matrix[M, K] X_test; # test features

}

parameters {
  vector[K] thetas; # coeffs for features
  real intercept;
}

model {
  thetas ~ normal(0, 10);
  intercept ~ normal(0, 10);
  
  y ~ bernoulli_logit(intercept + X * thetas);
}

generated quantities {
  array[M] int pred_y = bernoulli_logit_rng(intercept + X_test * thetas);
}


```

#### stan with matrix without prediction
accelarate the MCMC
```{stan output.var = "normal_withoutpred"}
data {
  int N; # number of train data
  int K; # number of features
  array[N] int y; # label of train
  matrix[N, K] X; # train features

}

parameters {
  vector[K] thetas; # coeffs for features
  real intercept;
}

model {
  for (i in 1:K) {
  thetas[i] ~ normal(0, 10);
}
  intercept ~ normal(0, 10);
  
  y ~ bernoulli_logit(intercept + X * thetas);
}
```
input the matrix
```{r}
input_matrix = list(
  N = 2000,
  K=12,
  y=df_train$Class,
  # X = as.matrix(select(df_train, -Class)) # original features
  X = X_train_scale # scaled features
)
```

```{r include=FALSE}
fit_without_pred <- sampling(
  normal_withoutpred,
  data = input_matrix,
  chains = 2,
  iter = 10000,
  seed=1469,
  warmup=2000)
```

still not mix well, Rhat large, probably need to rescale features?

```{r}
# visualize traceplot
traceplot(fit_without_pred, par="thetas")
```


```{r}
samples = extract(fit_without_pred)
posterior_Area = samples$thetas[,1]
plot(posterior_Area, type="o")
hist(posterior_Area)
```
```{r}
mcmc_areas(fit_without_pred, par= "thetas[9]")
```


What if we scale the features in preprocessing? If the values of coefficients are extreme, MCMC may not reach them in limited time.

After scaling the features, the slow-mixing seems to be addressed

```{r}
print(fit_without_pred)
```
Then we use the posterior mean of the coefficients to make the predictions on the test data (scaled)

```{r}
suppressMessages(require(boot))
fit_summary = summary(fit_without_pred)
coeffs = fit_summary$summary[1:13,"mean"]
X_test_scale_1 = cbind(X_test_scale, intercept=1)
y_pred = round(inv.logit(X_test_scale_1 %*% coeffs))
```
accuracy
```{r}
mean(y_pred == df_test$Class)
```
slightly better than frequentist lr. 

Let's visualize the 95% credible intervals
```{r}
suppressMessages(require(bayesplot))
thetas_name = c()
for (i in 1:12) {
  thetas_name[i] <- paste0("thetas[",i,"]")
}
mcmc_intervals(fit_without_pred, pars=c(thetas_name,"intercept"),prob = 0.95)
```
The credible interval of `Convex_Area` contains 0, while the CI from lr doesn't. The rest are similar (slightly different).



```{r}
mcmc_trace(fit_without_pred, pars=c(thetas_name,"intercept"))
```




## explore frequentist logistic regression
```{r}
lr = glm(Class~., family = 'binomial', df_train)
summary(lr)
```
```{r}
X_test = select(df_test, -Class)
y_pred = predict(lr, X_test, type = "response")
```
prediction accuracy
```{r}
mean(round(y_pred)==df_test$Class)
```

Baseline- the most frequent label
```{r}
1-mean(df_test$Class)
```

### Spike-and-slab
from previous, we find many CIs contains 0, can we perform feature selections?

```{stan output.var = "sas"}

data {
  int N; # number of train data
  int K; # number of features
  array[N] int y; # label of train
  matrix[N, K] X; # train features
  real v_1; #tune the shrinkage
  real v_0; 
}


parameters {
  vector[K] betas;
  real<lower=0> sigma_squared;
  #vector[K] gamma;
  vector<lower=0, upper=1>[K] ps; #probability to generate 0 or 1 for gamma
  #vector[K] as;
}

transformed parameters {
  #vector[K] as = (1-gamma)*0.01 + gamma * v_1;
  real<lower=0> sigma = sqrt(sigma_squared);
  vector[K] as;
  for (i in 1:K) {
    as[i] = ps[i]*v_1 + (1-ps[i])*v_0;
}

}

model {
  ps ~ beta(1, 1); #tried beta =k
  #gamma ~ bernoulli(theta);
  sigma_squared ~ inv_gamma(0.5,0.5);
  betas ~ normal(0, sigma*as);
  y ~ bernoulli_logit(X * betas);
}
```

```{r}
input_sas = list(
  N = 2000,
  K=12,
  y=df_train$Class,
  X = X_train_scale, # scaled features
  v_1= 1,
  v_0=0.01
)
```

```{r include=FALSE}
fit_sas <- sampling(
  sas,
  data = input_sas,
  chains = 2,
  iter = 10000,
  seed=1469,
  warmup=2000)
```

```{r}
suppressMessages(require(bayesplot))
betas_name = c()
for (i in 1:12) {
  betas_name[i] <- paste0("betas[",i,"]")
}
mcmc_intervals(fit_sas, pars=betas_name, prob = 0.95, prob_outer = 0.99)
```

```{r}
p_name = c()
for (i in 1:12) {
  p_name[i] <- paste0("ps[",i,"]")
}
mcmc_intervals(fit_sas, pars=p_name, prob = 0.95, prob_outer = 0.99)
```
```{r}
mcmc_trace(fit_sas, pars=betas_name)
```

```{r}
mcmc_trace(fit_sas, pars=p_name)
```
histogram
```{r}
mcmc_hist(fit_sas, betas_name)
```


```{r}
suppressMessages(require(boot))
fit_sas_summary = summary(fit_sas)
coeffs_sas = fit_sas_summary$summary[1:12,"mean"]
y_pred = round(inv.logit(X_test_scale %*% coeffs_sas))
mean(y_pred == df_test$Class)
```

We try to use the MCMC samples of coefficients in each iteration to get a sample for prediction by extracting samples.
```{r}
sas_samples <- extract(fit_sas)
```
betas
```{r}
beta_samples <- as.matrix(sas_samples$betas)
```
compute prediction via matrix operation
```{r}
prediction_original <- beta_samples %*% t(X_test_scale)
# 16000 rows, 500 cols, for each row we compute the prediction and accuracy
```
```{r}
prediction_boolean <- round(inv.logit(prediction_original))
```
defn accuracy fcn
```{r}
accuracy <- function(pred) {
  mean(pred == df_test$Class)
}
```

```{r}
accuracy_per_iter <- apply(prediction_boolean, MARGIN = 1, accuracy)
```

```{r}
max(accuracy_per_iter)
```
```{r}
min(accuracy_per_iter)
```
```{r}
plot(accuracy_per_iter[10000:16000], type='l')
```
```{r}
summary(accuracy_per_iter)
```
```{r}
quantile(accuracy_per_iter, c(0.05,0.95))
```




### laplace prior on the spike

We do observe a "little spike" and we want to make the selection effect stronger. Considering a laplace prior on the spike
```{stan output.var = "sas_laplace"}

data {
  int N; # number of train data
  int K; # number of features
  array[N] int y; # label of train
  matrix[N, K] X; # train features
  real v_1; #control slab
}


parameters {
  vector[K] slabs; #heavy tail normal
  vector<lower=0>[K] sigma_squared; #variance for each beta
  vector[K] spike; #laplace
  #vector[K] gamma;
  vector<lower=0, upper=1>[K] ps; #probability to generate 0 or 1 for gamma
  #vector[K] as;
}

transformed parameters {
  #vector[K] as = (1-gamma)*0.01 + gamma * v_1;
  vector<lower=0>[K] sigma = sqrt(sigma_squared);
  vector[K] betas;
  for (i in 1:K) {
    betas[i] = ps[i]*slabs[i] + (1-ps[i])*spike[i]; #mixture
}

}

model {
  ps ~ beta(1, 1); #tried beta =k
  #gamma ~ bernoulli(theta);
  sigma_squared ~ inv_gamma(0.5,0.5);
  for (i in 1:K) {
    spike[i] ~ double_exponential(0, sigma[i]);
    slabs[i] ~ normal(0, v_1*sigma[i]);
}
  y ~ bernoulli_logit(X * betas);
}
```


```{r}
input_laplace = list(
  N = 2000,
  K=12,
  y=df_train$Class,
  X = X_train_scale, # scaled features
  v_1=20
)
```

```{r include=FALSE}
fit_laplace <- sampling(
  sas_laplace,
  data = input_laplace,
  chains = 2,
  iter = 10000,
  seed=1469,
  warmup=2000)
```

```{r}
mcmc_trace(fit_laplace, pars=betas_name)
```














## diagnostic
### simulated data check - model specification
since the "spike" is not obvious enough, we add a simulated unrelated feature `random_feat` to the data, and we expect to see a spike posterior distribution.
```{r}
# create normal(0,1) feature with seed 213
set.seed(213)
X_simulate <- cbind(X_train_scale, random_feat=rnorm(2000))
```
fit the simulated feature with rstan
```{r}
input_simulate <- list(
  N = 2000,
  K=13,
  y=df_train$Class,
  X = X_simulate, # added random features
  v_1= 1,
  v_0=0.0001
)
```
mcmc
```{r include=FALSE}
fit_simulate <- sampling(
  sas,
  data = input_simulate,
  chains = 2,
  iter = 10000,
  seed=1469,
  warmup=2000)
```
result
```{r}
mcmc_areas(fit_simulate, par=c("betas[6]","betas[13]"))
```
We choose `Equiv_Diameter` and the `random_feat` to compare, we see the posterior of the `random_feat` is very "spiky" centered at 0, which implies the spike-and-slab prior does work to shrink irrelevant features to 0.

## comparison

```{r}
mcmc_areas(fit_without_pred, par= thetas_name[-c(8,9)], prob = 0.5, prob_outer = 0.95)
```
```{r}
mcmc_areas(fit_sas, par= betas_name[-c(8,9)], prob = 0.5, prob_outer = 0.95) +
  xlim(-40,20)
```
```{r}
mcmc_areas(fit_laplace, par= betas_name[-c(8,9)], prob = 0.5, prob_outer = 0.95) +
  xlim(-40,20)
```




```{r}
mcmc_areas(fit_simulate, par= "betas[13]")
```


```{r}
mcmc_areas(fit_sas, par= c(p_name))
```
```{r}
# explore normal and laplace
suppressMessages(require(extraDistr))
distr_prior <- data.frame(data=c(rnorm(2000,sd=0.01),rlaplace(2000,sigma = 0.01)),
                          name=factor(c(rep("normal",2000),rep("laplace",2000))))

ggplot(distr_prior,aes(x=data, fill=name)) +
  geom_histogram(bins=100) 
```


