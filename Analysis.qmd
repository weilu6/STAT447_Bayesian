---
title: "Analysis"
author: "Wei"
format: html
editor: visual
---

## EDA

Read data

```{r}
suppressMessages(require(readxl))
df = read_xlsx('proposal/Data/Pumpkin_Seeds_Dataset.xlsx')
```

Distribution of features

```{r}
suppressMessages(require(ggplot2))
suppressMessages(require(dplyr))
```

```{r}
df$Class <- factor(df$Class)
```

```{r}
features = colnames(df |> select(-"Class"))
```

```{r}
for (feat in features) {
  plot <- df |> ggplot(aes(x=.data[[feat]], fill=Class)) +
    geom_histogram(bins = 30,position = "identity", alpha=0.8) 
  print(plot)
}
```

```{r}
summary(df$Class)
```

### Summary:

-   The observed pumpkin seed classes are nearly balanced
-   All feature values are positive; the distributions are unimodal
-   Except for "Eccentricity", "Solidity", "Extent", the rest all symmetric
-   Generally, the feature values spread wider for "Ürgüp Sivrisi"
-   The disrtibutions of features are shifted between classes.

## preprocessing

The data size is 2500. To evaluate the prediction accuracy, we split the data set into train and test data first, we will do regression on the train data; compare the predictions on the test data
```{r}
# For the implementation of stan, we convert Class into integers;
# 0 for Çerçevelik
# 1 for Ürgüp Sivrisi
df$Class = as.integer(df$Class)-1
```

```{r}
set.seed(123)
trainIndex <- sample(seq_len(nrow(df)), size = 0.8 * nrow(df))
df_train <- df[trainIndex, ]
df_test <- df[-trainIndex, ]

dim(df_train)
dim(df_test)
```

Scaled features data
```{r}
X_train_scale <- scale(select(df_train, -Class))
X_test_scale <- scale(select(df_test, -Class))
```





## Baysian logistic regression with normal priors
Run MCMC
```{r}
suppressMessages(require(rstan))
```
```{r}
options(mc.cores=parallel::detectCores())
```

From EDA, the spread of each features are different, so we choose different priors on standard deviation for features. We use hierarhical model.(Too long to simulate)

#### stan with matrix with prediction
slow mixing, mis-specified
```{stan output.var = "normal_matrix"}
data {
  int N; # number of train data
  int M; # number of test data
  int K; # number of features
  array[N] int y; # label of train
  matrix[N, K] X; # train features
  matrix[M, K] X_test; # test features

}

parameters {
  vector[K] thetas; # coeffs for features
  real intercept;
}

model {
  thetas ~ normal(0, 10);
  intercept ~ normal(0, 10);
  
  y ~ bernoulli_logit(intercept + X * thetas);
}

generated quantities {
  array[M] int pred_y = bernoulli_logit_rng(intercept + X_test * thetas);
}


```

#### stan with matrix without prediction
accelarate the MCMC
```{stan output.var = "normal_withoutpred"}
data {
  int N; # number of train data
  int K; # number of features
  array[N] int y; # label of train
  matrix[N, K] X; # train features

}

parameters {
  vector[K] thetas; # coeffs for features
  real intercept;
}

model {
  for (i in 1:K) {
  thetas[i] ~ normal(0, 10);
}
  intercept ~ normal(0, 10);
  
  y ~ bernoulli_logit(intercept + X * thetas);
}
```
input the matrix
```{r}
input_matrix = list(
  N = 2000,
  K=12,
  y=df_train$Class,
  # X = as.matrix(select(df_train, -Class)) # original features
  X = X_train_scale # scaled features
)
```

```{r include=FALSE}
fit_without_pred <- sampling(
  normal_withoutpred,
  data = input_matrix,
  chains = 2,
  iter = 10000,
  seed=1469,
  warmup=2000)
```

still not mix well, Rhat large, probably need to rescale features?

```{r}
# visualize traceplot
traceplot(fit_without_pred, par="thetas")
```


```{r}
samples = extract(fit_without_pred)
posterior_Area = samples$thetas[,1]
plot(posterior_Area, type="o")
hist(posterior_Area)
```


What if we scale the features in preprocessing? If the values of coefficients are extreme, MCMC may not reach them in limited time.

After scaling the features, the slow-mixing seems to be addressed

```{r}
print(fit_without_pred)
```
Then we use the posterior mean of the coefficients to make the predictions on the test data (scaled)

```{r}
suppressMessages(require(boot))
fit_summary = summary(fit_without_pred)
coeffs = fit_summary$summary[1:13,"mean"]
X_test_scale_1 = cbind(X_test_scale, intercept=1)
y_pred = round(inv.logit(X_test_scale_1 %*% coeffs))
```
accuracy
```{r}
mean(y_pred == df_test$Class)
```
slightly better than frequentist lr. 

Let's visualize the 95% credible intervals
```{r}
suppressMessages(require(bayesplot))
thetas_name = c()
for (i in 1:12) {
  thetas_name[i] <- paste0("thetas[",i,"]")
}
mcmc_intervals(fit_without_pred, pars=c(thetas_name,"intercept"),prob = 0.95)
```
The credible interval of `Convex_Area` contains 0, while the CI from lr doesn't. The rest are similar (slightly different).



```{r}
mcmc_trace(fit_without_pred, pars=c(thetas_name,"intercept"))
```




## explore frequentist logistic regression
```{r}
lr = glm(Class~., family = 'binomial', df_train)
summary(lr)
```
```{r}
X_test = select(df_test, -Class)
y_pred = predict(lr, X_test, type = "response")
```
prediction accuracy
```{r}
mean(round(y_pred)==df_test$Class)
```

Baseline- the most frequent label
```{r}
1-mean(df_test$Class)
```

### Spike-and-slab
from previous, we find many CIs contains 0, can we perform feature selections?

```{stan output.var = "sas"}

data {
  int N; # number of train data
  int K; # number of features
  array[N] int y; # label of train
  matrix[N, K] X; # train features
  real v_1; #tune the shrinkage
  real v_0; 
}


parameters {
  vector[K] betas;
  real<lower=0> sigma_squared;
  #vector[K] gamma;
  vector<lower=0, upper=1>[K] ps; #probability to generate 0 or 1 for gamma
  #vector[K] as;
}

transformed parameters {
  #vector[K] as = (1-gamma)*0.01 + gamma * v_1;
  real<lower=0> sigma = sqrt(sigma_squared);
  vector[K] as;
  for (i in 1:K) {
    as[i] = ps[i]*v_1 + (1-ps[i])*v_0;
}

}

model {
  ps ~ beta(1, 1); #tried beta =k
  #gamma ~ bernoulli(theta);
  sigma_squared ~ inv_gamma(0.5,0.5);
  betas ~ normal(0, sigma*as);
  y ~ bernoulli_logit(X * betas);
}
```

```{r}
input_sas = list(
  N = 2000,
  K=12,
  y=df_train$Class,
  X = X_train_scale, # scaled features
  v_1= 1,
  v_0=0.0001
)
```

```{r include=FALSE}
fit_sas <- sampling(
  sas,
  data = input_sas,
  chains = 2,
  iter = 10000,
  seed=1469,
  warmup=2000)
```

```{r}
suppressMessages(require(bayesplot))
betas_name = c()
for (i in 1:12) {
  betas_name[i] <- paste0("betas[",i,"]")
}
mcmc_intervals(fit_sas, pars=betas_name, prob = 0.95, prob_outer = 0.99)
```

```{r}
p_name = c()
for (i in 1:12) {
  p_name[i] <- paste0("ps[",i,"]")
}
mcmc_intervals(fit_sas, pars=p_name, prob = 0.95, prob_outer = 0.99)
```
```{r}
mcmc_trace(fit_sas, pars=betas_name)
```

```{r}
mcmc_trace(fit_sas, pars=p_name)
```


```{r}
suppressMessages(require(boot))
fit_sas_summary = summary(fit_sas)
coeffs_sas = fit_sas_summary$summary[1:12,"mean"]
y_pred = round(inv.logit(X_test_scale %*% coeffs_sas))
mean(y_pred == df_test$Class)
```


