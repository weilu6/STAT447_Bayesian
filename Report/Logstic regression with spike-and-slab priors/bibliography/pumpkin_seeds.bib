@article{pumpkin,

author={Koklu,Murat and Sarigil,Seyma and Ozbek,Osman},

year={2021},

title={The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.)},

journal={Genetic resources and crop evolution},

volume={68},

number={7},

pages={2713-2726},

abstract={Pumpkin seeds are frequently consumed as confection worldwide because of their adequate amount of protein, fat, carbohydrate, and mineral contents. This study was carried out on the two most important and quality types of pumpkin seeds, “Ürgüp Sivrisi” and “Çerçevelik”, generally grown in Ürgüp and Karacaören regions in Turkey. However, morphological measurements of 2500 pumpkin seeds of both varieties were made possible by using the gray and binary forms of threshold techniques. Considering morphological features, all the data were modeled with five different machine learning methods: Logistic Regression (LR), Multilayer Perceptrons (MLP), Support Vector Machine (SVM) and Random Forest (RF), and k-Nearest Neighbor (k-NN), which further determined the most successful method for classifying pumpkin seed varieties. However, the performances of the models were determined with the help of the 10 k-fold cross-validation method. The accuracy rates of the classifiers were obtained as LR 87.92 percent, MLP 88.52 percent, SVM 88.64 percent, RF 87.56 percent, and k-NN 87.64 percent.;Pumpkin seeds are frequently consumed as confection worldwide because of their adequate amount of protein, fat, carbohydrate, and mineral contents. This study was carried out on the two most important and quality types of pumpkin seeds, "urgup Sivrisi" and "cercevelik", generally grown in urgup and Karacaoren regions in Turkey. However, morphological measurements of 2500 pumpkin seeds of both varieties were made possible by using the gray and binary forms of threshold techniques. Considering morphological features, all the data were modeled with five different machine learning methods: Logistic Regression (LR), Multilayer Perceptrons (MLP), Support Vector Machine (SVM) and Random Forest (RF), and k-Nearest Neighbor (k-NN), which further determined the most successful method for classifying pumpkin seed varieties. However, the performances of the models were determined with the help of the 10 k-fold cross-validation method. The accuracy rates of the classifiers were obtained as LR 87.92 percent, MLP 88.52 percent, SVM 88.64 percent, RF 87.56 percent, and k-NN 87.64 percent.;Pumpkin seeds are frequently consumed as confection worldwide because of their adequate amount of protein, fat, carbohydrate, and mineral contents. This study was carried out on the two most important and quality types of pumpkin seeds, “Ürgüp Sivrisi” and “Çerçevelik”, generally grown in Ürgüp and Karacaören regions in Turkey. However, morphological measurements of 2500 pumpkin seeds of both varieties were made possible by using the gray and binary forms of threshold techniques. Considering morphological features, all the data were modeled with five different machine learning methods: Logistic Regression (LR), Multilayer Perceptrons (MLP), Support Vector Machine (SVM) and Random Forest (RF), and k-Nearest Neighbor (k-NN), which further determined the most successful method for classifying pumpkin seed varieties. However, the performances of the models were determined with the help of the 10 k-fold cross-validation method. The accuracy rates of the classifiers were obtained as LR 87.92 percent, MLP 88.52 percent, SVM 88.64 percent, RF 87.56 percent, and k-NN 87.64 percent.;},

keywords={Agriculture; Agronomy; Biomedical and Life Sciences; Carbohydrates; Classification; Cucurbita pepo; Learning algorithms; Life Sciences; Life Sciences & Biomedicine; Logistic regression; Machine learning; Morphology; Multilayer peceptrons; Multilayer perceptrons; Plant Genetics and Genomics; Plant Physiology; Plant Sciences; Plant Systematics/Taxonomy/Biogeography; Pumpkin seed; Random forest; Review; Science & Technology; Seeds; Support vector machine; Support vector machines; Thresholding},

isbn={0925-9864},

language={English},

}

@article{selection,

author={George,Edward I. and McCulloch,Robert E.},

year={1993},

title={Variable Selection via Gibbs Sampling},

journal={Journal of the American Statistical Association},

volume={88},

number={423},

pages={881-889},

abstract={A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability-the promising ones-can then be identified by their more frequent appearance in the Gibbs sample.;A procedure is proposed and developed that uses probabilistic considerations for selecting promising subsets. Those subsets with higher probability can be identified by their more frequent appearance in the Gibbs sample.;},

keywords={Ambivalence; Data augmentation; Exact sciences and technology; Hierarchical Bayes; High frequencies; Induced substructures; Latent variables; Least squares; Linear inference, regression; Linear models; Linear regression; Mathematics; Mixture; Modeling; Multilevel models; Multiple regression; Probabilities; Probability; Probability and statistics; Regression; Regression analysis; Sampling; Sciences and techniques of general use; Single Equation Models, Single Variables: Other (C29); Statistics; Theory and Methods},

isbn={0162-1459},

language={English},

}

@article{sas,

author={Ishwaran,Hemant and Rao,J. S.},

year={2005},

title={Spike and Slab Variable Selection: Frequentist and Bayesian Strategies},

journal={The Annals of statistics},

volume={33},

number={2},

pages={730-773},

abstract={Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty.;Variable selection in the linear regression model takes many apparent faces from both frequenist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theorectically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainity in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainity. [PUBLLICATION ABSTRACT];},

keywords={62J05; 62J07; Consistent estimators; Coordinate systems; Estimators; Frequentism; Generalized ridge regression; hypervariance; Mathematical models; Mathematics; model averaging; model uncertainty; Modeling; Oracles; ordinary least squares; Orthogonality; penalization; Physical Sciences; Regression analysis; Regression Methodology; rescaling; Science & Technology; shrinkage; Statistics & Probability; stochastic variable selection; Studies; Variable coefficients; Zcut; Zero},

isbn={0090-5364},

language={English},

}

@article{diag,

author={Gelman,A. and Goegebeur,Y. and Tuerlinckx,F. and Van Mechelen,I.},

year={2000},

title={Diagnostic checks for discrete data regression models using posterior predictive simulations},

journal={Applied statistics},

volume={49},

number={2},

pages={247-268},

abstract={Model checking with discrete data regressions can be difficult because the usual methods such as residual plots have complicated reference distributions that depend on the parameters in the model. Posterior predictive checks have been proposed as a Bayesian way to average the results of goodness-of-fit tests in the presence of uncertainty in estimation of the parameters. We try this approach using a variety of discrepancy variables for generalized linear models fitted to a historical data set on behavioural learning. We then discuss the general applicability of our findings in the context of a recent applied example on which we have worked. We find that the following discrepancy variables work well, in the sense of being easy to interpret and sensitive to important model failures: structured displays of the entire data set, general discrepancy variables based on plots of binned or smoothed residuals versus predictors and specific discrepancy variables created on the basis of the particular concerns arising in an application. Plots of binned residuals are especially easy to use because their predictive distributions under the model are sufficiently simple that model checks can often be made implicitly. The following discrepancy variables did not work well: scatterplots of latent residuals defined from an underlying continuous model and quantile–quantile plots of these residuals.},

keywords={Applied sciences; Artificial intelligence; Bayesian statistics; Binary regression; Computer science; control theory; systems; Data analysis; Data models; Datasets; Exact sciences and technology; Experimentation; Forecasts; Generalized linear models; Learning; Learning and adaptive systems; Linear inference, regression; Logistic regression; Logistics; Mathematics; Modeling; Parametric inference; Parametric models; Physical Sciences; Predictive modeling; Probability and statistics; Quantile-quantile plots; Realized discrepancies; Regression analysis; Residual plots; Science & Technology; Sciences and techniques of general use; Sequential design; Sequential methods; Simulation; Simulations; Statistics; Statistics & Probability; Stochastic learning models; Stochastic processes},

isbn={0035-9254;1467-9876;},

language={English},

}